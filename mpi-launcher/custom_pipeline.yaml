apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: launch-kubeflow-mpi-job-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.22, pipelines.kubeflow.org/pipeline_compilation_time: '2024-09-20T16:24:43.178588',
    pipelines.kubeflow.org/pipeline_spec: '{"description": "An example to launch deepspeed.",
      "inputs": [{"default": "", "name": "namespace", "optional": true, "type": "String"},
      {"default": "", "name": "application_id", "optional": true, "type": "String"},
      {"default": "", "name": "queue", "optional": true, "type": "String"}, {"default":
      "", "name": "img", "optional": true, "type": "String"}, {"default": "", "name":
      "cmd", "optional": true, "type": "String"}, {"default": "3", "name": "num_worker",
      "optional": true, "type": "Integer"}, {"default": "20", "name": "cpu_per_worker",
      "optional": true, "type": "Integer"}, {"default": "80", "name": "memory_per_worker",
      "optional": true, "type": "Integer"}, {"default": "1", "name": "gpu_per_worker",
      "optional": true, "type": "Integer"}, {"default": "1", "name": "node_group_id",
      "optional": true, "type": "Integer"}, {"default": "", "name": "node_type", "optional":
      true, "type": "String"}, {"default": "", "name": "public_pvc_nm", "optional":
      true, "type": "String"}, {"default": "", "name": "public_vol_nm", "optional":
      true, "type": "String"}, {"default": "", "name": "public_vol_mnt_path", "optional":
      true, "type": "String"}, {"default": "", "name": "private_pvc_nm", "optional":
      true, "type": "String"}, {"default": "", "name": "private_vol_nm", "optional":
      true, "type": "String"}, {"default": "", "name": "private_vol_mnt_path", "optional":
      true, "type": "String"}, {"default": "", "name": "exp_nm", "optional": true,
      "type": "String"}, {"default": "", "name": "run_name", "optional": true, "type":
      "String"}, {"default": "", "name": "config_map_name", "optional": true, "type":
      "String"}, {"default": "", "name": "device", "optional": true, "type": "String"},
      {"default": "", "name": "value", "optional": true, "type": "String"}, {"default":
      "", "name": "nccl_conf", "optional": true, "type": "String"}, {"default": "",
      "name": "email", "optional": true, "type": "String"}, {"default": "", "name":
      "username", "optional": true, "type": "String"}], "name": "launch-kubeflow-mpi-job"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.22}
spec:
  entrypoint: launch-kubeflow-mpi-job
  templates:
  - name: clear-mpi-job
    container:
      args: [--name, violet-run-pipeline-mpi-e5e9310af5844389, --namespace, '{{inputs.parameters.namespace}}',
        --num-worker, '{{inputs.parameters.num_worker}}', --version, v1]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'kubernetes' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'kubernetes' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def Clear_MPI_Job(name, namespace, num_worker, version="v1"):
            import kubernetes
            kubernetes.config.load_incluster_config()
            api_instance_custom = kubernetes.client.CustomObjectsApi()
            api_instance_core = kubernetes.client.CoreV1Api()
            group = "kubeflow.org"
            plural = "mpijobs"
            try:
                api_response = api_instance_custom.delete_namespaced_custom_object(
                    name=name,
                    namespace=namespace,
                    group=group,
                    version=version,
                    plural=plural,
                    body=kubernetes.client.models.V1DeleteOptions(
                        propagation_policy='Foreground',
                        grace_period_seconds=15
                    )
                )
                print("MPI-Job %s/%s deleted. Status='%s'" % (name, namespace, str(api_response.get("status", None))))
            except kubernetes.client.rest.ApiException as e:
                print("Exception when calling CustomObjectsApi->delete_namespaced_custom_object: %s\n" % e)

            try:
                api_response = api_instance_custom.delete_namespaced_custom_object(
                    name=name,
                    namespace=namespace,
                    group="scheduling.x-k8s.io",
                    version="v1alpha1",
                    plural="podgroups",
                    body=kubernetes.client.models.V1DeleteOptions(
                        propagation_policy='Foreground',
                        grace_period_seconds=15
                    )
                )
                print("PodGroup %s/%s deleted. Status='%s'" % (name, namespace, str(api_response.get("status", None))))
            except kubernetes.client.rest.ApiException as e:
                print("Exception when calling CustomObjectsApi->delete_namespaced_custom_object: %s\n" % e)

            try:
                api_response = api_instance_core.delete_namespaced_service(name=f"{name}-launcher", namespace=namespace, body={"propagationPolicy": "Foreground"})
                print("Service %s/%s deleted. Status='%s'" % (f"{name}-launcher", namespace, str(api_response.status)))
                for index in range(int(num_worker)):
                    api_response = api_instance_core.delete_namespaced_service(name=f"{name}-worker-{index}", namespace=namespace, body={"propagationPolicy": "Foreground"})
                    print("Service %s/%s deleted. Status='%s'" % (f"{name}-worker-{index}", namespace, str(api_response.status)))
            except kubernetes.client.rest.ApiException as e:
                print("Exception when calling CoreV1Api->delete_namespaced_service: %s\n" % e)

            try:
                api_response = api_instance_core.delete_namespaced_service_account(name=f"{name}-launcher", namespace=namespace, body={"propagationPolicy": "Foreground"})
                print("ServiceAccount %s/%s deleted. Status='%s'" % (f"{name}-launcher", namespace, str({'conditions': None, 'load_balancer': {'ingress': None}})))
            except kubernetes.client.rest.ApiException as e:
                print("Exception when calling CoreV1Api->delete_namespaced_service_account: %s\n" % e)

        import argparse
        _parser = argparse.ArgumentParser(prog='Clear MPI Job', description='')
        _parser.add_argument("--name", dest="name", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--namespace", dest="namespace", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--num-worker", dest="num_worker", type=str, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--version", dest="version", type=str, required=False, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = Clear_MPI_Job(**_parsed_args)
      image: python:3.7
    inputs:
      parameters:
      - {name: namespace}
      - {name: num_worker}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--name", {"inputValue": "name"}, "--namespace", {"inputValue":
          "namespace"}, "--num-worker", {"inputValue": "num_worker"}, {"if": {"cond":
          {"isPresent": "version"}, "then": ["--version", {"inputValue": "version"}]}}],
          "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip
          install --quiet --no-warn-script-location ''kubernetes'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''kubernetes''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def Clear_MPI_Job(name, namespace, num_worker, version=\"v1\"):\n    import
          kubernetes\n    kubernetes.config.load_incluster_config()\n    api_instance_custom
          = kubernetes.client.CustomObjectsApi()\n    api_instance_core = kubernetes.client.CoreV1Api()\n    group
          = \"kubeflow.org\"\n    plural = \"mpijobs\"\n    try:\n        api_response
          = api_instance_custom.delete_namespaced_custom_object(\n            name=name,\n            namespace=namespace,\n            group=group,\n            version=version,\n            plural=plural,\n            body=kubernetes.client.models.V1DeleteOptions(\n                propagation_policy=''Foreground'',\n                grace_period_seconds=15\n            )\n        )\n        print(\"MPI-Job
          %s/%s deleted. Status=''%s''\" % (name, namespace, str(api_response.get(\"status\",
          None))))\n    except kubernetes.client.rest.ApiException as e:\n        print(\"Exception
          when calling CustomObjectsApi->delete_namespaced_custom_object: %s\\n\"
          % e)\n\n    try:\n        api_response = api_instance_custom.delete_namespaced_custom_object(\n            name=name,\n            namespace=namespace,\n            group=\"scheduling.x-k8s.io\",\n            version=\"v1alpha1\",\n            plural=\"podgroups\",\n            body=kubernetes.client.models.V1DeleteOptions(\n                propagation_policy=''Foreground'',\n                grace_period_seconds=15\n            )\n        )\n        print(\"PodGroup
          %s/%s deleted. Status=''%s''\" % (name, namespace, str(api_response.get(\"status\",
          None))))\n    except kubernetes.client.rest.ApiException as e:\n        print(\"Exception
          when calling CustomObjectsApi->delete_namespaced_custom_object: %s\\n\"
          % e)\n\n    try:\n        api_response = api_instance_core.delete_namespaced_service(name=f\"{name}-launcher\",
          namespace=namespace, body={\"propagationPolicy\": \"Foreground\"})\n        print(\"Service
          %s/%s deleted. Status=''%s''\" % (f\"{name}-launcher\", namespace, str(api_response.status)))\n        for
          index in range(int(num_worker)):\n            api_response = api_instance_core.delete_namespaced_service(name=f\"{name}-worker-{index}\",
          namespace=namespace, body={\"propagationPolicy\": \"Foreground\"})\n            print(\"Service
          %s/%s deleted. Status=''%s''\" % (f\"{name}-worker-{index}\", namespace,
          str(api_response.status)))\n    except kubernetes.client.rest.ApiException
          as e:\n        print(\"Exception when calling CoreV1Api->delete_namespaced_service:
          %s\\n\" % e)\n\n    try:\n        api_response = api_instance_core.delete_namespaced_service_account(name=f\"{name}-launcher\",
          namespace=namespace, body={\"propagationPolicy\": \"Foreground\"})\n        print(\"ServiceAccount
          %s/%s deleted. Status=''%s''\" % (f\"{name}-launcher\", namespace, str({''conditions'':
          None, ''load_balancer'': {''ingress'': None}})))\n    except kubernetes.client.rest.ApiException
          as e:\n        print(\"Exception when calling CoreV1Api->delete_namespaced_service_account:
          %s\\n\" % e)\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Clear
          MPI Job'', description='''')\n_parser.add_argument(\"--name\", dest=\"name\",
          type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--namespace\",
          dest=\"namespace\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--num-worker\",
          dest=\"num_worker\", type=str, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--version\",
          dest=\"version\", type=str, required=False, default=argparse.SUPPRESS)\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = Clear_MPI_Job(**_parsed_args)\n"],
          "image": "python:3.7"}}, "inputs": [{"name": "name"}, {"name": "namespace"},
          {"name": "num_worker"}, {"default": "v1", "name": "version", "optional":
          true}], "name": "Clear MPI Job"}', pipelines.kubeflow.org/component_ref: '{}',
        pipelines.kubeflow.org/arguments.parameters: '{"name": "violet-run-pipeline-mpi-e5e9310af5844389",
          "namespace": "{{inputs.parameters.namespace}}", "num_worker": "{{inputs.parameters.num_worker}}",
          "version": "v1"}'}
  - name: exit-handler-1
    inputs:
      parameters:
      - {name: cmd}
      - {name: config_map_name}
      - {name: cpu_per_worker}
      - {name: device}
      - {name: email}
      - {name: exp_nm}
      - {name: gpu_per_worker}
      - {name: img}
      - {name: memory_per_worker}
      - {name: namespace}
      - {name: nccl_conf}
      - {name: node_group_id}
      - {name: node_type}
      - {name: num_worker}
      - {name: private_pvc_nm}
      - {name: private_vol_mnt_path}
      - {name: private_vol_nm}
      - {name: public_pvc_nm}
      - {name: public_vol_mnt_path}
      - {name: public_vol_nm}
      - {name: run_name}
      - {name: username}
      - {name: value}
    dag:
      tasks:
      - name: logging-op
        template: logging-op
        dependencies: [mpi-job-launcher]
        arguments:
          parameters:
          - {name: config_map_name, value: '{{inputs.parameters.config_map_name}}'}
          - {name: exp_nm, value: '{{inputs.parameters.exp_nm}}'}
          - {name: run_name, value: '{{inputs.parameters.run_name}}'}
      - name: mpi-job-launcher
        template: mpi-job-launcher
        arguments:
          parameters:
          - {name: cmd, value: '{{inputs.parameters.cmd}}'}
          - {name: config_map_name, value: '{{inputs.parameters.config_map_name}}'}
          - {name: cpu_per_worker, value: '{{inputs.parameters.cpu_per_worker}}'}
          - {name: device, value: '{{inputs.parameters.device}}'}
          - {name: email, value: '{{inputs.parameters.email}}'}
          - {name: exp_nm, value: '{{inputs.parameters.exp_nm}}'}
          - {name: gpu_per_worker, value: '{{inputs.parameters.gpu_per_worker}}'}
          - {name: img, value: '{{inputs.parameters.img}}'}
          - {name: memory_per_worker, value: '{{inputs.parameters.memory_per_worker}}'}
          - {name: namespace, value: '{{inputs.parameters.namespace}}'}
          - {name: nccl_conf, value: '{{inputs.parameters.nccl_conf}}'}
          - {name: node_group_id, value: '{{inputs.parameters.node_group_id}}'}
          - {name: node_type, value: '{{inputs.parameters.node_type}}'}
          - {name: num_worker, value: '{{inputs.parameters.num_worker}}'}
          - {name: private_pvc_nm, value: '{{inputs.parameters.private_pvc_nm}}'}
          - {name: private_vol_mnt_path, value: '{{inputs.parameters.private_vol_mnt_path}}'}
          - {name: private_vol_nm, value: '{{inputs.parameters.private_vol_nm}}'}
          - {name: public_pvc_nm, value: '{{inputs.parameters.public_pvc_nm}}'}
          - {name: public_vol_mnt_path, value: '{{inputs.parameters.public_vol_mnt_path}}'}
          - {name: public_vol_nm, value: '{{inputs.parameters.public_vol_nm}}'}
          - {name: run_name, value: '{{inputs.parameters.run_name}}'}
          - {name: username, value: '{{inputs.parameters.username}}'}
          - {name: value, value: '{{inputs.parameters.value}}'}
  - name: launch-kubeflow-mpi-job
    inputs:
      parameters:
      - {name: cmd}
      - {name: config_map_name}
      - {name: cpu_per_worker}
      - {name: device}
      - {name: email}
      - {name: exp_nm}
      - {name: gpu_per_worker}
      - {name: img}
      - {name: memory_per_worker}
      - {name: namespace}
      - {name: nccl_conf}
      - {name: node_group_id}
      - {name: node_type}
      - {name: num_worker}
      - {name: private_pvc_nm}
      - {name: private_vol_mnt_path}
      - {name: private_vol_nm}
      - {name: public_pvc_nm}
      - {name: public_vol_mnt_path}
      - {name: public_vol_nm}
      - {name: run_name}
      - {name: username}
      - {name: value}
    dag:
      tasks:
      - name: exit-handler-1
        template: exit-handler-1
        arguments:
          parameters:
          - {name: cmd, value: '{{inputs.parameters.cmd}}'}
          - {name: config_map_name, value: '{{inputs.parameters.config_map_name}}'}
          - {name: cpu_per_worker, value: '{{inputs.parameters.cpu_per_worker}}'}
          - {name: device, value: '{{inputs.parameters.device}}'}
          - {name: email, value: '{{inputs.parameters.email}}'}
          - {name: exp_nm, value: '{{inputs.parameters.exp_nm}}'}
          - {name: gpu_per_worker, value: '{{inputs.parameters.gpu_per_worker}}'}
          - {name: img, value: '{{inputs.parameters.img}}'}
          - {name: memory_per_worker, value: '{{inputs.parameters.memory_per_worker}}'}
          - {name: namespace, value: '{{inputs.parameters.namespace}}'}
          - {name: nccl_conf, value: '{{inputs.parameters.nccl_conf}}'}
          - {name: node_group_id, value: '{{inputs.parameters.node_group_id}}'}
          - {name: node_type, value: '{{inputs.parameters.node_type}}'}
          - {name: num_worker, value: '{{inputs.parameters.num_worker}}'}
          - {name: private_pvc_nm, value: '{{inputs.parameters.private_pvc_nm}}'}
          - {name: private_vol_mnt_path, value: '{{inputs.parameters.private_vol_mnt_path}}'}
          - {name: private_vol_nm, value: '{{inputs.parameters.private_vol_nm}}'}
          - {name: public_pvc_nm, value: '{{inputs.parameters.public_pvc_nm}}'}
          - {name: public_vol_mnt_path, value: '{{inputs.parameters.public_vol_mnt_path}}'}
          - {name: public_vol_nm, value: '{{inputs.parameters.public_vol_nm}}'}
          - {name: run_name, value: '{{inputs.parameters.run_name}}'}
          - {name: username, value: '{{inputs.parameters.username}}'}
          - {name: value, value: '{{inputs.parameters.value}}'}
  - name: logging-op
    container:
      command: [sh, -c, python mlflow_run_detail.py]
      env:
      - {name: MLFLOW_EXPERIMENT_NAME, value: '{{inputs.parameters.exp_nm}}'}
      - {name: MLFLOW_RUN_NAME, value: '{{inputs.parameters.run_name}}'}
      envFrom:
      - configMapRef: {name: '{{inputs.parameters.config_map_name}}', optional: true}
      image: docker.io/yhjh5302/mlflow-logging:v1
    inputs:
      parameters:
      - {name: config_map_name}
      - {name: exp_nm}
      - {name: run_name}
    outputs:
      artifacts:
      - {name: mlpipeline-ui-metadata, path: /mlpipeline-ui-metadata.json}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  - name: mpi-job-launcher
    container:
      args:
      - --name
      - violet-run-pipeline-mpi-e5e9310af5844389
      - --namespace
      - '{{inputs.parameters.namespace}}'
      - --version
      - v1
      - --launcherSpec
      - |-
        {               "replicas": 1,               "restartPolicy": "Never",               "template": {                 "metadata": {                   "annotations": {                     "sidecar.istio.io/inject": "false",                     "aiplatform/owner": "{{inputs.parameters.email}}"                   },                   "labels": {                     "aiplatform/task-parallelism": "multi-node",                     "aiplatform/task-type": "deepspeed",                     "app": "yunikorn",                     "scheduling.x-k8s.io/pod-group": "violet-run-pipeline-mpi-e5e9310af5844389"                   }                 },                 "spec": {                   "affinity": {                     "nodeAffinity": {                       "requiredDuringSchedulingIgnoredDuringExecution": {                         "nodeSelectorTerms": [{                           "matchExpressions": [                             {                               "key": "aiplatform/node-group-id",                               "operator": "In",                               "values": ["{{inputs.parameters.node_group_id}}"]                             },                             {                               "key": "aiplatform/node-type",                               "operator": "In",                               "values": ["{{inputs.parameters.node_type}}"]                             }                           ]                         }]                       }                     }                   },                   "containers": [                     {                       "name": "deepspeed",                       "image": "{{inputs.parameters.img}}",                       "command": ["/bin/bash", "-c", "sed -i'' -e's/^#   StrictHostKeyChecking ask/   StrictHostKeyChecking no/' /etc/ssh/ssh_config \
         && sed -i'' -e's/^#PermitRootLogin prohibit-password$/PermitRootLogin yes/' /etc/ssh/sshd_config \
         && sed -i'' -e's/^#PasswordAuthentication yes$/PasswordAuthentication no/' /etc/ssh/sshd_config \
         && sed -i'' -e's/^#PermitUserEnvironment no$/PermitUserEnvironment yes/' /etc/ssh/sshd_config \
         && sed -i'' -e's/^#PermitEmptyPasswords no$/PermitEmptyPasswords no/' /etc/ssh/sshd_config \
         && sed -i'' -e's/^UsePAM yes/UsePAM no/' /etc/ssh/sshd_config \
         && /usr/sbin/sshd && {{inputs.parameters.cmd}}"],                       "envFrom": [{                         "configMapRef": {                           "name": "{{inputs.parameters.config_map_name}}",                           "optional": true                         }                       }],                       "env": [                         {                           "name": "node_group_id",                           "value": "{{inputs.parameters.node_group_id}}"                         },                         {                           "name": "node_type",                           "value": "{{inputs.parameters.node_type}}"                         },                         {                           "name": "{{inputs.parameters.device}}",                           "value": "{{inputs.parameters.value}}"                         },                         {                           "name": "MLFLOW_EXPERIMENT_NAME",                           "value": "{{inputs.parameters.exp_nm}}"                         },                         {                           "name": "MLFLOW_RUN_NAME",                           "value": "{{inputs.parameters.run_name}}"                         }                       ],                       "resources": {                         "limits": {                           "cpu": {{inputs.parameters.cpu_per_worker}},                           "memory": {{inputs.parameters.memory_per_worker}},                           "nvidia.com/gpu": {{inputs.parameters.gpu_per_worker}},                           "rdma/rdma_shared_device_ndr": 1                         }                       },                       "volumeMounts": [                         {                           "name": "{{inputs.parameters.public_vol_nm}}",                           "mountPath": "{{inputs.parameters.public_vol_mnt_path}}"                         },                         {                           "name": "{{inputs.parameters.private_vol_nm}}",                           "mountPath": "{{inputs.parameters.private_vol_mnt_path}}"                         },                         {                           "name": "dshm",                           "mountPath": "/dev/shm"                         },                         {                           "name": "nccl-conf",                           "subPath": "nccl.conf",                           "mountPath": "/etc/nccl.conf"                         },                         {                           "name": "ssh-public-key",                           "subPath": "authorized_keys",                           "mountPath": "/root/.ssh/authorized_keys"                         },                         {                           "name": "ssh-private-key",                           "subPath": "id_rsa_violet",                           "mountPath": "/root/.ssh/id_rsa"                         }                       ],                       "securityContext": {                         "capabilities": {                           "add": [                             "IPC_LOCK"                           ]                         }                       }                     }                   ],                   "volumes": [                     {                       "name": "{{inputs.parameters.public_vol_nm}}",                       "persistentVolumeClaim": { "claimName": "{{inputs.parameters.public_pvc_nm}}" }                     },                     {                       "name": "{{inputs.parameters.private_vol_nm}}",                       "persistentVolumeClaim": { "claimName": "{{inputs.parameters.private_pvc_nm}}" }                     },                     {                       "name": "dshm",                       "emptyDir": {                         "medium": "Memory"                       }                     },                     {                       "name": "nccl-conf",                       "configMap": {                         "name": "{{inputs.parameters.nccl_conf}}",                         "defaultMode": 420                       }                     },                     {                       "name": "ssh-public-key",                       "secret": {                         "secretName": "{{inputs.parameters.username}}-ssh-key",                         "defaultMode": 384,                         "items": [                           {                             "key": "public_key",                             "path": "authorized_keys"                           }                         ]                       }                     },                     {                       "name": "ssh-private-key",                       "secret": {                         "secretName": "{{inputs.parameters.username}}-ssh-key",                         "defaultMode": 384,                         "items": [                           {                             "key": "private_key",                             "path": "id_rsa_violet"                           }                         ]                       }                     }                   ],                   "schedulerName": "scheduler-plugins-scheduler"                 }               }             }
      - --workerSpec
      - |-
        {               "replicas": {{inputs.parameters.num_worker}},               "restartPolicy": "Never",               "template": {                 "metadata": {                   "annotations": {                     "sidecar.istio.io/inject": "false",                     "aiplatform/owner": "{{inputs.parameters.email}}"                   },                   "labels": {                     "aiplatform/task-parallelism": "multi-node",                     "aiplatform/task-type": "deepspeed",                     "app": "yunikorn",                     "scheduling.x-k8s.io/pod-group": "violet-run-pipeline-mpi-e5e9310af5844389"                   }                 },                 "spec": {                   "affinity": {                     "nodeAffinity": {                       "requiredDuringSchedulingIgnoredDuringExecution": {                         "nodeSelectorTerms": [{                           "matchExpressions": [                             {                               "key": "aiplatform/node-group-id",                               "operator": "In",                               "values": ["{{inputs.parameters.node_group_id}}"]                             },                             {                               "key": "aiplatform/node-type",                               "operator": "In",                               "values": ["{{inputs.parameters.node_type}}"]                             }                           ]                         }]                       }                     }                   },                   "containers": [                     {                       "name": "deepspeed",                       "image": "{{inputs.parameters.img}}",                       "command": ["/bin/bash", "-c", "sed -i'' -e's/^#   StrictHostKeyChecking ask/   StrictHostKeyChecking no/' /etc/ssh/ssh_config \
         && sed -i'' -e's/^#PermitRootLogin prohibit-password$/PermitRootLogin yes/' /etc/ssh/sshd_config \
         && sed -i'' -e's/^#PasswordAuthentication yes$/PasswordAuthentication no/' /etc/ssh/sshd_config \
         && sed -i'' -e's/^#PermitUserEnvironment no$/PermitUserEnvironment yes/' /etc/ssh/sshd_config \
         && sed -i'' -e's/^#PermitEmptyPasswords no$/PermitEmptyPasswords no/' /etc/ssh/sshd_config \
         && sed -i'' -e's/^UsePAM yes/UsePAM no/' /etc/ssh/sshd_config \
         && /usr/sbin/sshd && sleep infinity"],                       "envFrom": [{                         "configMapRef": {                           "name": "{{inputs.parameters.config_map_name}}",                           "optional": true                         }                       }],                       "env": [                         {                           "name": "node_group_id",                           "value": "{{inputs.parameters.node_group_id}}"                         },                         {                           "name": "node_type",                           "value": "{{inputs.parameters.node_type}}"                         },                         {                           "name": "{{inputs.parameters.device}}",                           "value": "{{inputs.parameters.value}}"                         },                         {                           "name": "MLFLOW_EXPERIMENT_NAME",                           "value": "{{inputs.parameters.exp_nm}}"                         },                         {                           "name": "MLFLOW_RUN_NAME",                           "value": "{{inputs.parameters.run_name}}"                         }                       ],                       "resources": {                         "limits": {                           "cpu": {{inputs.parameters.cpu_per_worker}},                           "memory": {{inputs.parameters.memory_per_worker}},                           "nvidia.com/gpu": {{inputs.parameters.gpu_per_worker}},                           "rdma/rdma_shared_device_ndr": 1                         }                       },                       "volumeMounts": [                         {                           "name": "{{inputs.parameters.public_vol_nm}}",                           "mountPath": "{{inputs.parameters.public_vol_mnt_path}}"                         },                         {                           "name": "{{inputs.parameters.private_vol_nm}}",                           "mountPath": "{{inputs.parameters.private_vol_mnt_path}}"                         },                         {                           "name": "dshm",                           "mountPath": "/dev/shm"                         },                         {                           "name": "nccl-conf",                           "subPath": "nccl.conf",                           "mountPath": "/etc/nccl.conf"                         },                         {                           "name": "ssh-public-key",                           "subPath": "authorized_keys",                           "mountPath": "/root/.ssh/authorized_keys"                         },                         {                           "name": "ssh-private-key",                           "subPath": "id_rsa_violet",                           "mountPath": "/root/.ssh/id_rsa"                         }                       ],                       "securityContext": {                         "capabilities": {                           "add": [                             "IPC_LOCK"                           ]                         }                       }                     }                   ],                   "volumes": [                     {                       "name": "{{inputs.parameters.public_vol_nm}}",                       "persistentVolumeClaim": { "claimName": "{{inputs.parameters.public_pvc_nm}}" }                     },                     {                       "name": "{{inputs.parameters.private_vol_nm}}",                       "persistentVolumeClaim": { "claimName": "{{inputs.parameters.private_pvc_nm}}" }                     },                     {                       "name": "dshm",                       "emptyDir": {                         "medium": "Memory"                       }                     },                     {                       "name": "nccl-conf",                       "configMap": {                         "name": "{{inputs.parameters.nccl_conf}}",                         "defaultMode": 420                       }                     },                     {                       "name": "ssh-public-key",                       "secret": {                         "secretName": "{{inputs.parameters.username}}-ssh-key",                         "defaultMode": 384,                         "items": [                           {                             "key": "public_key",                             "path": "authorized_keys"                           }                         ]                       }                     },                     {                       "name": "ssh-private-key",                       "secret": {                         "secretName": "{{inputs.parameters.username}}-ssh-key",                         "defaultMode": 384,                         "items": [                           {                             "key": "private_key",                             "path": "id_rsa_violet"                           }                         ]                       }                     }                   ],                   "schedulerName": "scheduler-plugins-scheduler"                 }               }             }
      - --scheduleTimeoutSeconds
      - '1440'
      - --schedulingPolicy
      - '{}'
      - --cleanPodPolicy
      - Running
      command: [python, /ml/launch_mpi_job.py]
      image: yhjh5302/kubeflow-mpi-job-launcher:v1
    inputs:
      parameters:
      - {name: cmd}
      - {name: config_map_name}
      - {name: cpu_per_worker}
      - {name: device}
      - {name: email}
      - {name: exp_nm}
      - {name: gpu_per_worker}
      - {name: img}
      - {name: memory_per_worker}
      - {name: namespace}
      - {name: nccl_conf}
      - {name: node_group_id}
      - {name: node_type}
      - {name: num_worker}
      - {name: private_pvc_nm}
      - {name: private_vol_mnt_path}
      - {name: private_vol_nm}
      - {name: public_pvc_nm}
      - {name: public_vol_mnt_path}
      - {name: public_vol_nm}
      - {name: run_name}
      - {name: username}
      - {name: value}
      artifacts:
      - name: delete_after_done
        path: /tmp/inputs/delete_after_done/data
        raw:
          data: "True"
      - name: job_timeout_minutes
        path: /tmp/inputs/job_timeout_minutes/data
        raw: {data: '1440'}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.22
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"description": "Kubeflow
          MPIJob launcher", "implementation": {"container": {"args": ["--name", {"inputValue":
          "name"}, "--namespace", {"inputValue": "namespace"}, "--version", {"inputValue":
          "version"}, "--launcherSpec", {"inputValue": "launcher_spec"}, "--workerSpec",
          {"inputValue": "worker_spec"}, "--scheduleTimeoutSeconds", {"inputValue":
          "schedule_timeout_seconds"}, "--schedulingPolicy", {"inputValue": "scheduling_policy"},
          "--cleanPodPolicy", {"inputValue": "clean_pod_policy"}, {"if": {"cond":
          {"isPresent": "active_deadline_seconds"}, "then": ["--activeDeadlineSeconds",
          {"inputValue": "active_deadline_seconds"}]}}, {"if": {"cond": {"isPresent":
          "backoff_limit"}, "then": ["--backoffLimit", {"inputValue": "backoff_limit"}]}},
          {"if": {"cond": {"isPresent": "ttl_seconds_after_finished"}, "then": ["--ttlSecondsAfterFinished",
          {"inputValue": "ttl_seconds_after_finished"}]}}], "command": ["python",
          "/ml/launch_mpi_job.py"], "image": "yhjh5302/kubeflow-mpi-job-launcher:v1"}},
          "inputs": [{"description": "MPIJob name.", "name": "name", "type": "String"},
          {"default": "kubeflow", "description": "MPIJob namespace (likely your current
          namespace).", "name": "namespace", "type": "String"}, {"default": "v1",
          "description": "MPIJob version.", "name": "version", "type": "String"},
          {"default": "{}", "description": "MPIJob Launcher replicaSpecs.", "name":
          "launcher_spec", "type": "JsonObject"}, {"default": "{}", "description":
          "MPIJob Worker replicaSpecs.", "name": "worker_spec", "type": "JsonObject"},
          {"default": 1440, "description": "Time in seconds to wait for the job to
          schedule.", "name": "schedule_timeout_seconds", "type": "Integer"}, {"default":
          "{}", "description": "MPIJob supports the gang-scheduling.", "name": "scheduling_policy",
          "type": "JsonObject"}, {"default": 1440, "description": "Time in minutes
          to wait for the job to complete.", "name": "job_timeout_minutes", "type":
          "Integer"}, {"default": "True", "description": "Whether to delete the job
          after it is finished.", "name": "delete_after_done", "type": "Boolean"},
          {"default": "Running", "description": "Defines the policy for cleaning up
          pods after the MPIJob completes.", "name": "clean_pod_policy", "type": "String"},
          {"description": "Specifies the duration (in seconds) since startTime during
          which the job can remain active before it is terminated. Must be a positive
          integer. This setting applies only to pods where restartPolicy is OnFailure
          or Always.", "name": "active_deadline_seconds", "optional": true, "type":
          "Integer"}, {"description": "Number of retries before marking this job as
          failed.", "name": "backoff_limit", "optional": true, "type": "Integer"},
          {"description": "Defines the TTL for cleaning up finished MPIJobs.", "name":
          "ttl_seconds_after_finished", "optional": true, "type": "Integer"}], "name":
          "MPI-Job Launcher"}', pipelines.kubeflow.org/component_ref: '{"digest":
          "3a6fc91a4fe1dfa94bf57452a021802979231337cffbb4e04d19394bfecb4c50", "url":
          "./mpi_job_component.yaml"}', pipelines.kubeflow.org/arguments.parameters: '{"clean_pod_policy":
          "Running", "launcher_spec": "{               \"replicas\": 1,               \"restartPolicy\":
          \"Never\",               \"template\": {                 \"metadata\": {                   \"annotations\":
          {                     \"sidecar.istio.io/inject\": \"false\",                     \"aiplatform/owner\":
          \"{{inputs.parameters.email}}\"                   },                   \"labels\":
          {                     \"aiplatform/task-parallelism\": \"multi-node\",                     \"aiplatform/task-type\":
          \"deepspeed\",                     \"app\": \"yunikorn\",                     \"scheduling.x-k8s.io/pod-group\":
          \"violet-run-pipeline-mpi-e5e9310af5844389\"                   }                 },                 \"spec\":
          {                   \"affinity\": {                     \"nodeAffinity\":
          {                       \"requiredDuringSchedulingIgnoredDuringExecution\":
          {                         \"nodeSelectorTerms\": [{                           \"matchExpressions\":
          [                             {                               \"key\": \"aiplatform/node-group-id\",                               \"operator\":
          \"In\",                               \"values\": [\"{{inputs.parameters.node_group_id}}\"]                             },                             {                               \"key\":
          \"aiplatform/node-type\",                               \"operator\": \"In\",                               \"values\":
          [\"{{inputs.parameters.node_type}}\"]                             }                           ]                         }]                       }                     }                   },                   \"containers\":
          [                     {                       \"name\": \"deepspeed\",                       \"image\":
          \"{{inputs.parameters.img}}\",                       \"command\": [\"/bin/bash\",
          \"-c\", \"sed -i'''' -e''s/^#   StrictHostKeyChecking ask/   StrictHostKeyChecking
          no/'' /etc/ssh/ssh_config \\\n && sed -i'''' -e''s/^#PermitRootLogin prohibit-password$/PermitRootLogin
          yes/'' /etc/ssh/sshd_config \\\n && sed -i'''' -e''s/^#PasswordAuthentication
          yes$/PasswordAuthentication no/'' /etc/ssh/sshd_config \\\n && sed -i''''
          -e''s/^#PermitUserEnvironment no$/PermitUserEnvironment yes/'' /etc/ssh/sshd_config
          \\\n && sed -i'''' -e''s/^#PermitEmptyPasswords no$/PermitEmptyPasswords
          no/'' /etc/ssh/sshd_config \\\n && sed -i'''' -e''s/^UsePAM yes/UsePAM no/''
          /etc/ssh/sshd_config \\\n && /usr/sbin/sshd && {{inputs.parameters.cmd}}\"],                       \"envFrom\":
          [{                         \"configMapRef\": {                           \"name\":
          \"{{inputs.parameters.config_map_name}}\",                           \"optional\":
          true                         }                       }],                       \"env\":
          [                         {                           \"name\": \"node_group_id\",                           \"value\":
          \"{{inputs.parameters.node_group_id}}\"                         },                         {                           \"name\":
          \"node_type\",                           \"value\": \"{{inputs.parameters.node_type}}\"                         },                         {                           \"name\":
          \"{{inputs.parameters.device}}\",                           \"value\": \"{{inputs.parameters.value}}\"                         },                         {                           \"name\":
          \"MLFLOW_EXPERIMENT_NAME\",                           \"value\": \"{{inputs.parameters.exp_nm}}\"                         },                         {                           \"name\":
          \"MLFLOW_RUN_NAME\",                           \"value\": \"{{inputs.parameters.run_name}}\"                         }                       ],                       \"resources\":
          {                         \"limits\": {                           \"cpu\":
          {{inputs.parameters.cpu_per_worker}},                           \"memory\":
          {{inputs.parameters.memory_per_worker}},                           \"nvidia.com/gpu\":
          {{inputs.parameters.gpu_per_worker}},                           \"rdma/rdma_shared_device_ndr\":
          1                         }                       },                       \"volumeMounts\":
          [                         {                           \"name\": \"{{inputs.parameters.public_vol_nm}}\",                           \"mountPath\":
          \"{{inputs.parameters.public_vol_mnt_path}}\"                         },                         {                           \"name\":
          \"{{inputs.parameters.private_vol_nm}}\",                           \"mountPath\":
          \"{{inputs.parameters.private_vol_mnt_path}}\"                         },                         {                           \"name\":
          \"dshm\",                           \"mountPath\": \"/dev/shm\"                         },                         {                           \"name\":
          \"nccl-conf\",                           \"subPath\": \"nccl.conf\",                           \"mountPath\":
          \"/etc/nccl.conf\"                         },                         {                           \"name\":
          \"ssh-public-key\",                           \"subPath\": \"authorized_keys\",                           \"mountPath\":
          \"/root/.ssh/authorized_keys\"                         },                         {                           \"name\":
          \"ssh-private-key\",                           \"subPath\": \"id_rsa_violet\",                           \"mountPath\":
          \"/root/.ssh/id_rsa\"                         }                       ],                       \"securityContext\":
          {                         \"capabilities\": {                           \"add\":
          [                             \"IPC_LOCK\"                           ]                         }                       }                     }                   ],                   \"volumes\":
          [                     {                       \"name\": \"{{inputs.parameters.public_vol_nm}}\",                       \"persistentVolumeClaim\":
          { \"claimName\": \"{{inputs.parameters.public_pvc_nm}}\" }                     },                     {                       \"name\":
          \"{{inputs.parameters.private_vol_nm}}\",                       \"persistentVolumeClaim\":
          { \"claimName\": \"{{inputs.parameters.private_pvc_nm}}\" }                     },                     {                       \"name\":
          \"dshm\",                       \"emptyDir\": {                         \"medium\":
          \"Memory\"                       }                     },                     {                       \"name\":
          \"nccl-conf\",                       \"configMap\": {                         \"name\":
          \"{{inputs.parameters.nccl_conf}}\",                         \"defaultMode\":
          420                       }                     },                     {                       \"name\":
          \"ssh-public-key\",                       \"secret\": {                         \"secretName\":
          \"{{inputs.parameters.username}}-ssh-key\",                         \"defaultMode\":
          384,                         \"items\": [                           {                             \"key\":
          \"public_key\",                             \"path\": \"authorized_keys\"                           }                         ]                       }                     },                     {                       \"name\":
          \"ssh-private-key\",                       \"secret\": {                         \"secretName\":
          \"{{inputs.parameters.username}}-ssh-key\",                         \"defaultMode\":
          384,                         \"items\": [                           {                             \"key\":
          \"private_key\",                             \"path\": \"id_rsa_violet\"                           }                         ]                       }                     }                   ],                   \"schedulerName\":
          \"scheduler-plugins-scheduler\"                 }               }             }",
          "name": "violet-run-pipeline-mpi-e5e9310af5844389", "namespace": "{{inputs.parameters.namespace}}",
          "schedule_timeout_seconds": "1440", "scheduling_policy": "{}", "version":
          "v1", "worker_spec": "{               \"replicas\": {{inputs.parameters.num_worker}},               \"restartPolicy\":
          \"Never\",               \"template\": {                 \"metadata\": {                   \"annotations\":
          {                     \"sidecar.istio.io/inject\": \"false\",                     \"aiplatform/owner\":
          \"{{inputs.parameters.email}}\"                   },                   \"labels\":
          {                     \"aiplatform/task-parallelism\": \"multi-node\",                     \"aiplatform/task-type\":
          \"deepspeed\",                     \"app\": \"yunikorn\",                     \"scheduling.x-k8s.io/pod-group\":
          \"violet-run-pipeline-mpi-e5e9310af5844389\"                   }                 },                 \"spec\":
          {                   \"affinity\": {                     \"nodeAffinity\":
          {                       \"requiredDuringSchedulingIgnoredDuringExecution\":
          {                         \"nodeSelectorTerms\": [{                           \"matchExpressions\":
          [                             {                               \"key\": \"aiplatform/node-group-id\",                               \"operator\":
          \"In\",                               \"values\": [\"{{inputs.parameters.node_group_id}}\"]                             },                             {                               \"key\":
          \"aiplatform/node-type\",                               \"operator\": \"In\",                               \"values\":
          [\"{{inputs.parameters.node_type}}\"]                             }                           ]                         }]                       }                     }                   },                   \"containers\":
          [                     {                       \"name\": \"deepspeed\",                       \"image\":
          \"{{inputs.parameters.img}}\",                       \"command\": [\"/bin/bash\",
          \"-c\", \"sed -i'''' -e''s/^#   StrictHostKeyChecking ask/   StrictHostKeyChecking
          no/'' /etc/ssh/ssh_config \\\n && sed -i'''' -e''s/^#PermitRootLogin prohibit-password$/PermitRootLogin
          yes/'' /etc/ssh/sshd_config \\\n && sed -i'''' -e''s/^#PasswordAuthentication
          yes$/PasswordAuthentication no/'' /etc/ssh/sshd_config \\\n && sed -i''''
          -e''s/^#PermitUserEnvironment no$/PermitUserEnvironment yes/'' /etc/ssh/sshd_config
          \\\n && sed -i'''' -e''s/^#PermitEmptyPasswords no$/PermitEmptyPasswords
          no/'' /etc/ssh/sshd_config \\\n && sed -i'''' -e''s/^UsePAM yes/UsePAM no/''
          /etc/ssh/sshd_config \\\n && /usr/sbin/sshd && sleep infinity\"],                       \"envFrom\":
          [{                         \"configMapRef\": {                           \"name\":
          \"{{inputs.parameters.config_map_name}}\",                           \"optional\":
          true                         }                       }],                       \"env\":
          [                         {                           \"name\": \"node_group_id\",                           \"value\":
          \"{{inputs.parameters.node_group_id}}\"                         },                         {                           \"name\":
          \"node_type\",                           \"value\": \"{{inputs.parameters.node_type}}\"                         },                         {                           \"name\":
          \"{{inputs.parameters.device}}\",                           \"value\": \"{{inputs.parameters.value}}\"                         },                         {                           \"name\":
          \"MLFLOW_EXPERIMENT_NAME\",                           \"value\": \"{{inputs.parameters.exp_nm}}\"                         },                         {                           \"name\":
          \"MLFLOW_RUN_NAME\",                           \"value\": \"{{inputs.parameters.run_name}}\"                         }                       ],                       \"resources\":
          {                         \"limits\": {                           \"cpu\":
          {{inputs.parameters.cpu_per_worker}},                           \"memory\":
          {{inputs.parameters.memory_per_worker}},                           \"nvidia.com/gpu\":
          {{inputs.parameters.gpu_per_worker}},                           \"rdma/rdma_shared_device_ndr\":
          1                         }                       },                       \"volumeMounts\":
          [                         {                           \"name\": \"{{inputs.parameters.public_vol_nm}}\",                           \"mountPath\":
          \"{{inputs.parameters.public_vol_mnt_path}}\"                         },                         {                           \"name\":
          \"{{inputs.parameters.private_vol_nm}}\",                           \"mountPath\":
          \"{{inputs.parameters.private_vol_mnt_path}}\"                         },                         {                           \"name\":
          \"dshm\",                           \"mountPath\": \"/dev/shm\"                         },                         {                           \"name\":
          \"nccl-conf\",                           \"subPath\": \"nccl.conf\",                           \"mountPath\":
          \"/etc/nccl.conf\"                         },                         {                           \"name\":
          \"ssh-public-key\",                           \"subPath\": \"authorized_keys\",                           \"mountPath\":
          \"/root/.ssh/authorized_keys\"                         },                         {                           \"name\":
          \"ssh-private-key\",                           \"subPath\": \"id_rsa_violet\",                           \"mountPath\":
          \"/root/.ssh/id_rsa\"                         }                       ],                       \"securityContext\":
          {                         \"capabilities\": {                           \"add\":
          [                             \"IPC_LOCK\"                           ]                         }                       }                     }                   ],                   \"volumes\":
          [                     {                       \"name\": \"{{inputs.parameters.public_vol_nm}}\",                       \"persistentVolumeClaim\":
          { \"claimName\": \"{{inputs.parameters.public_pvc_nm}}\" }                     },                     {                       \"name\":
          \"{{inputs.parameters.private_vol_nm}}\",                       \"persistentVolumeClaim\":
          { \"claimName\": \"{{inputs.parameters.private_pvc_nm}}\" }                     },                     {                       \"name\":
          \"dshm\",                       \"emptyDir\": {                         \"medium\":
          \"Memory\"                       }                     },                     {                       \"name\":
          \"nccl-conf\",                       \"configMap\": {                         \"name\":
          \"{{inputs.parameters.nccl_conf}}\",                         \"defaultMode\":
          420                       }                     },                     {                       \"name\":
          \"ssh-public-key\",                       \"secret\": {                         \"secretName\":
          \"{{inputs.parameters.username}}-ssh-key\",                         \"defaultMode\":
          384,                         \"items\": [                           {                             \"key\":
          \"public_key\",                             \"path\": \"authorized_keys\"                           }                         ]                       }                     },                     {                       \"name\":
          \"ssh-private-key\",                       \"secret\": {                         \"secretName\":
          \"{{inputs.parameters.username}}-ssh-key\",                         \"defaultMode\":
          384,                         \"items\": [                           {                             \"key\":
          \"private_key\",                             \"path\": \"id_rsa_violet\"                           }                         ]                       }                     }                   ],                   \"schedulerName\":
          \"scheduler-plugins-scheduler\"                 }               }             }"}'}
  arguments:
    parameters:
    - {name: namespace, value: ''}
    - {name: application_id, value: ''}
    - {name: queue, value: ''}
    - {name: img, value: ''}
    - {name: cmd, value: ''}
    - {name: num_worker, value: '3'}
    - {name: cpu_per_worker, value: '20'}
    - {name: memory_per_worker, value: '80'}
    - {name: gpu_per_worker, value: '1'}
    - {name: node_group_id, value: '1'}
    - {name: node_type, value: ''}
    - {name: public_pvc_nm, value: ''}
    - {name: public_vol_nm, value: ''}
    - {name: public_vol_mnt_path, value: ''}
    - {name: private_pvc_nm, value: ''}
    - {name: private_vol_nm, value: ''}
    - {name: private_vol_mnt_path, value: ''}
    - {name: exp_nm, value: ''}
    - {name: run_name, value: ''}
    - {name: config_map_name, value: ''}
    - {name: device, value: ''}
    - {name: value, value: ''}
    - {name: nccl_conf, value: ''}
    - {name: email, value: ''}
    - {name: username, value: ''}
  serviceAccountName: pipeline-runner
  onExit: clear-mpi-job
